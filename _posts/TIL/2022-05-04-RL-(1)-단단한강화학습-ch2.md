---
title: "RL 단단한 강화학습 Ch2 (1)"
categories:
    - TIL
tags:
    - CS
    - Reinforcement Learning
---
# Ch2 다중 선택

## 2.1 다중 선택 문제

- 활용과 탐험 사이의 정교한 균형을 맞추는 것은 중요하다.
- 좋은 방법이 있더라도 강화학습을 적용하는데 있어서 성립하지 않으면 쓸 수 없다.
- 여기서는 주로 슬롯 머신을 예로 든다. 여러 개의 레버가 있고, 각 레버를 당기면 다른 보상이 나오는 머신을 생각하면 된다.

## 2.2 행동 가치 방법 (Action-Value Method)

- ***각 행동이 얼마나 이로운지를 정확히 안다면, 어떤 행동을 해야하는지 파악할 수 있을 것이라는 아이디어***에서 출발한다.
    - 따라서 각 행동이 얼마나 이로운지(value)를 알아내는 방법이다.
- **Value(Q) 추정**
    - 다양한 방법이 있지만, 대표적으로 대수의 법칙을 활용한 sample-average가 있다. 그냥 time step마다 return의 표본평균 구하는거다. 이 것이 곧 실제 참 값으로 수렴할 것이니 말이다.
- **Action 채택**
    - Greedy하게 가장 높은 value를 갖는 action만을 채택할 수도 있다.
    - 다만, value가 가장 높지 않더라도 실제로는 더 높은 return을 주는 action이 있을 수도 있다. 따라서 입실론의 확률로 랜덤한 action을 채택하는 입실론-greedy방법도 있다. 결정론적 상황이더라도, non-stationary(시간에 따라 return 분포가 변한다던지)한 문제에서는 탐험이 필요하므로 이러한 방법을 써야할 것이다.

## 2.4 점증적 구현

- time step이 매우 많아지면, 언제 다 return을 다시 더해서 표본 평균을 구하겠나.
    - $Q_{n+1} = Q_n + {1 \over n+1}(R_n-Q_n)$($Q$는 추정하고자 하는 return의 참값. 대수의 법칙에 의해서 계속해서 update해가는 값!)과 같이 풀어 쓸 수 있다. 이러면 한번에 계산 가능!
    - return과 value의 차이를 error라고 지칭한다.  Return은 우리가 추정하고 있는 Q의 참값의 지침, 방향이 되므로, target이라고 하기도 한다.

## 2.5 비정상 문제의 흔적

Non-stationary : 정적이지 않은 것. 시간에 따라 변하는 것

- 문제가 stationary하다면 return의 분포가 변하지 않겠지만, non-stationary 문제에서는 그렇지 않다. 이 때는, 최근의 return에 더 큰 가중치를 두는 것이 타당할 것이다.
- sample average에서는 $Q_{n+1} = Q_n + \alpha(R_n-Q_n)$에서 $\alpha, [0, 1]$를 고정하는 것으로 이를 해결할 수 있다.
    - $= \alpha R_n + (1-\alpha)Q_n = \alpha R_n + (1-\alpha)(\alpha R_{n-1}+(1-\alpha)Q_{n-1})$ = ...
- 확률론적 근사 이론에 따르면 이 경우 수렴하지 않는데, 실제 문제에 적용하거나 경험에 기반한 실증적 연구를 진행할 때는 이게 낫다. 고정하지 않더라도 수렴속도가 어차피 겁나 느리다고.

## 2.6 긍정적 초기값

- $Q_1(a)$를 실제 return의 평균으로 예측되는거보다 훨씬 높게 잡는다면, 초반에 탐험을 많이할 수 밖에 없다.(조금만 생각해봐라. 당연한 이야기다)
- 실제로도 그렇긴한데, non-stationary문제에서는 적합하지 않다. 탐험을 야기시키는 원동력이 일시적이기 때문이다. non-stationary문제에서는 수행 과제들이 시간이 지나면서 변할 때 탐험을 상황에 알맞게 계속 해야할 것이기 때문이다.
- 그럼에도 불구하고, 실제 상황에서는 생각보다 이렇게 간단한 방법 여러개를 적절히 섞어서 쓰는 경우가 있다고 한다. 그냥 염두해두자

## 2.7 신뢰 상한(Upper Confidence Bound) 행동 선택

- 입실론 그리디 행동 선택은 비탐욕적 행동 선택을 강요하지만, 탐욕적, 비탐욕적 선택 사이에 특별한 선호도 없이 선택을 한다.
- 하지만 당연히 실제 최적 선택이 될만한 가능성이 높은 것들 중에 탐험하는게 더 바람직할 것이다.
- $A_t \ \dot= \ argmax_a[Q_t(a)+c\sqrt{ln (t)] \over N_t(a)}$
    - $N_t, c$는 각각 time step t까지 a가 선택된 횟수와 탐험의 정도를 나타낸다.
    - 제곱근 항을 통해 a의 가치에 대한 추정값의 불확실성 또는 편차를 고려하는 방법이다.
    - a가 선택될 때마다 제곱근항의 분모가 커져 불확실성이 감소하며, 그렇지 않을 때 분자는 계속 커져가므로 불확실성이 증가한다. 자연로그를 사용해 무한히 증가하지만, 증가량은 시간에 따라 감소함을 표현했다.
- 비정상적 문제를 다루는데 어려움이 꽤 있다고 한다. 다양한 방법들이 있으며, 실용성과 정확도?의 trade-off가 존재하며, 상황에 따라 이러한 것들도 바뀐다 정도로 인지해두자.

## 2.8 경사도 다중 선택 알고리즘

- 가치를 추정하고, 이를 통해 행동을 선택하는 방법도 있었지만, 각 행동에 대한 선호도를 학습하는 방법도 소개한다.
    - 선호도는 보상과는 조금 다르다. 각 행동이 다른 행동에 대해 갖는 상대적 선호도가 중요하다.
    - 모든 행동 선호도에 100을 추가한다고 해도, 행동을 선택할 확률에 영향을 미치지 않는다. 무슨말인지 나도 읽으면서 좀 헷갈렸다. 아래를 보자
    - $Pr\{A_t = a\}\ \dot=\ {e^{H_t(a)}\over \sum^k_{b=1}e^{H_t(b)}}=\pi_t(a)$
    - $H_t(a), \pi(a)$는 각각 a의 선호도와, a가 선택될 확률이다. 표기법은 그냥 그렇구나 하고 받아들이면 된다.
    - 그냥 소프트맥스 함수이다. 모든 행동 선호도에 상수를 추가해도 저 확률은 안바뀐다.
    - 어쨋든 이런거 써서도 행동 선택한다고 하는데, 선호도 update는 다음과 같이 한다.
        - t에서 선택된 행동 A는
        $H_{t+1}(A_t) \ \dot= \ H(A_t)+\alpha(R_t -\bar{R_t})(1-\pi_t(A_t))$
        - 그 외의 모든 행동 a는 ($a \neq A_t)$
        $H_{t+1}(a) \ \dot= \ H(a)-\alpha(R_t -\bar{R_t})\pi_t(a)$
        - $\bar{R_t}$는 time step t까지의 모든 보상에 대한 평균이다. 이거보다 이번 time step에서의 return이 크면 행동 A의 선호도를 올리고, 나머지 행동들 a에 대해서는 내리는 방식. 왜 $\pi$를 가중치로 곱하는지는 모르겠는데 stochastic gradient ascent랑 관련된거인듯? 모르겠당!
    - 경사도 증가에 대한 확률론적 근사로 이해하면 더 깊은 이해를 할 수 있다고 한다. 나중에 다시 돌아와서 보자
    

## 2.9 연관 탐색(맥락적 다중 선택)

Associative search(contextual bandits)

- 언급은 안했지만 책에서 슬롯 머신 문제(다중 선택 문제. 선택지가 여러개인 문제)를 계속해서 예로 들었다. 하나의 슬롯 머신에서 최선의 선택을 하려면 가장 보상이 높은 버튼이나 슬롯을 누르는게 이상적일 것이다.
- ***강화학습 문제에서는 이렇게 하나의 슬롯 머신(상황)만 있는게 아니다. 강화학습 문제에서는 여러 상황이 존재***하며, 행동들을 상황에 연관시킬 필요가 있다. 각 상황이 특별한 특징을 갖는다고 하면, 우리는 상황을 구별하고, 이에 맞게 행동의 가치를 생각해볼 수 있다. 이러한 것은 바로 위에서 $\pi$로 표현된 “정책”에 의해 표현될 수 있다. 차근차근 알아보자
- 참고로 연관 탐색, 맥락적 다중 선택은 시행착오로 최고의 행동을 탐색하는 동시에 이 행동이 최고가 되는 상황을 이 행동과 연관시키기 때문에 붙혀진 이름이다.
- 연관 탐색 문제는 다중 선택 문제와 강화학습 중간 어딘가에 있다. ***각 행동이 현재의 보상에만 영향을 준다면 연관 탐색 문제, 그게 아니라 다음 상황에도 영향을 줄 수 있다면 강화학습 문제가 된다.***

## ETC

내 나름대로 다시 봤을 때 이해를 돕기 위해 설명을 보충한다. 연습 2.10을 보고 좋은 예시가 떠올라 참고해서 작성했다. 대충 아래와 같은 경우로 설명할 수 있는것 같다.

- A, B버튼이 있는 머신을 생각해보자. 각 버튼을 누르면 카지노 머신처럼 돈(보상)이 나온다.
1. 나오는 돈의 양이 고정되어 있거나, 확률이 고정된 경우
    - 소개된 value update, action 선택 알고리즘에 따라서 참값을 추정하고, 가장 높은 참값을 갖는 버튼을 누를 것
2. 나오는 돈의 양이나 확률이 시간에 따라 변하는 경
    - non-stationary한 특성을 고려해서 참값을 추정하고, 버튼을 누를 것
3. 나오는 돈의 양이나 확률이 두가지 케이스로 정해져 있는 경우
    1. 현재 어떤 케이스에 해당하는지 모르는 경우
        - 서로 다른 여러개의 다중 선택 문제인 셈이다. non-stationary 문제처럼 보이지만 참값이 서서히 변하지 않는다면 현재까지 소개된 방법을 적용하는데 무리가 있을 것이다. 효과가 좋지 않을 것
    2. 현재 어떤 케이스에 해당하는지 알 수 없는 경우
        - 각 케이스(상태)를 인식하고, 각 상태에서의 각 버튼의 보상 참값을 추정, 가장 높은 참값을 갖는 버튼을 누르면 될 것이다.

3-b의 케이스에서 각 상태에서의 각 행동이 다음 상태에도 영향을 주거나, 연관된다면, 그 것이 곧 강화학습에서 다루는 문제가 될 것이다. 물론 A, B버튼과 두가지 케이스로 한정된 위 예시와 달리 강화학습에서 상태와 행동은 매우 많을 수 있다.