---
title: "RL 단단한 강화학습 Ch2 (1)"
categories:
    - TIL
tags:
    - CS
    - Reinforcement Learning
---
# Ch2

한달 쯤 전에 대충 읽고 긴가민가하다가 지금 다시 읽으니까 다 이해가 잘 된다. 또 잊기전에 대충 정리
단단한 강화학습 읽어나가고, 구현 따로

## 2.1 다중 선택 문제

- 활용과 탐험 사이의 정교한 균형을 맞추는 것은 중요하다.
- 좋은 방법이 있더라도 강화학습을 적용하는데 있어서 성립하지 않으면 쓸 수 없다.

## 2.2 행동 가치 방법 (Action-Value Method)

- 각 행동이 얼마나 이로운지를 정확히 안다면, 어떤 행동을 해야하는지 파악할 수 있을 것이라는 아이디어에서 출발한다.
    - 따라서 각 행동이 얼마나 이로운지(value)를 알아내는 방법이다.
- Value(Q) 추정
    - 다양한 방법이 있지만, 대표적으로 대수의 법칙을 활용한 sample-average가 있다. 그냥 time step마다 return의 표본평균 구하는거다. 이 것이 곧 실제 참 값으로 수렴할 것이니 말이다.
- Action 채택
    - Greedy하게 가장 높은 value를 갖는 action만을 채택할 수도 있다.
    - 다만, value가 가장 높지 않더라도 실제로는 더 높은 return을 주는 action이 있을 수도 있다. 따라서 입실론의 확률로 랜덤한 action을 채택하는 입실론-greedy방법도 있다. 결정론적 상황이더라도, non-stationary(시간에 따라 return 분포가 변한다던지)한 문제에서는 탐험이 필요하므로 이러한 방법을 써야할 것이다.

## 2.4 점증적 구현

- time step이 매우 많아지면, 언제 다 return을 다시 더해서 표본 평균을 구하겠나.
    - $Q_{n+1} = Q_n + {1 \over n}(R_n-Q_n)$과 같이 풀어 쓸 수 있다. 이러면 한번에 계산 가능!
    - return과 value의 차이를 error라고 지칭한다.  Return은 우리가 추정하고 있는 Q의 참값의 지침, 방향이 되므로, target이라고 하기도 한다.

## 2.5 비정상 문제의 흔적

Non-stationary : 정적이지 않은 것. 시간에 따라 변하는 것

- 문제가 stationary하다면 return의 분포가 변하지 않겠지만, non-stationary 문제에서는 그렇지 않다. 이 때는, 최근의 return에 더 큰 가중치를 두는 것이 타당할 것이다.
- $Q_{n+1} = Q_n + \alpha(R_n-Q_n)$에서 $\alpha, [0, 1]$를 고정하는 것으로 이를 해결할 수 있다.
    - $= \alpha R_n + (1-\alpha)Q_n = \alpha R_n + (1-\alpha)(\alpha R_{n-1}+(1-\alpha)Q_{n-1})$ = ...
- 확률론적 근사 이론에 따르면 이 경우 수렴하지 않는데, 실제 문제에 적용하거나 경험에 기반한 실증적 연구를 진행할 때는 이게 낫다. 고정하지 않더라도 수렴속도가 어차피 겁나 느리다고.

## 2.6 긍정적 초기값

- $Q_1(a)$를 실제 return의 평균으로 예측되는거보다 훨씬 높게 잡는다면, 초반에 탐험을 많이할 수 밖에 없다.(조금만 생각해봐라. 당연한 이야기다)
- 실제로도 그렇긴한데, non-stationary문제에서는 적합하지 않다. 탐험을 야기시키는 원동력이 일시적이기 때문이다. non-stationary문제에서는 수행 과제들이 시간이 지나면서 변할 때 탐험을 상황에 알맞게 계속 해야할 것이기 때문이다.
- 그럼에도 불구하고, 실제 상황에서는 생각보다 이렇게 간단한 방법 여러개를 적절히 섞어서 쓰는 경우가 있다고 한다. 그냥 염두해두자