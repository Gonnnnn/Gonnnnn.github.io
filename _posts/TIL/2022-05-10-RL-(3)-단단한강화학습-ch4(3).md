---
title: "RL 단단한 강화학습 Ch4 (3)"
categories:
    - TIL
tags:
    - CS
    - Reinforcement Learning
---
# Ch4 DP

책 외에 구글링 해보고 글을 읽는 부분이 많았다. 특히 “파이썬과 케라스로 배우는 강화학습”을 참고했다. 간단하게만 적어봤다.
https://github.com/rlcode/reinforcement-learning-kr.git 의 코드 훑어보며 이해를 도왔다.

## 4.1 Policy Iteration - Policy Evaluation

- 이제껏 해왔듯, MDP를 통해 정의된 문제에서 시작한다.
- value function는 policy에 의존했었다. 하나의 policy에서 value function의 참값을 알아내는 과정이다.
- Bellman expected equation을 dynamic programming으로 풀어내 value function의 참값을 알아낸다.

## 4.2 Policy Improvement

- Value function의 참값을 알아낸 후, policy를 갱신한다.
- 각 state에서 value를 최대화하는 action을 선택하는, greedy한 방법을 따라 갱신한다.

## 4.3 Policy Iteration

- Policy Iteration은 Policy evaluation과 policy improvement를 반복해 최적 value function, policy를 얻어내는 방법이다.

## 4.4 Value Iteration

- Policy iteration과 달리, value function을 갱신할 때 bellman optimal equation을 사용한다.
- 가장 큰 value를 갖게 하는 action만을 가지고 update하는 것이다.
- 애초에 현재 value function이 optimal하다는 가정을 기반으로 시작한다. 이렇게 무한히 반복하면 optimal한 value function에 도달한다는 것이다.(이런 부분들에 대한 증명 등은 생략한다. 잘 모른다 나도)
- 따라서 단순히 action value function을 계산해 가장 큰 값을 갖게하는 action을 고르면, 그게 곧 policy가 된다.

## 4.7 DP의 효율성

- 많이들 알고 있는 부분들이다.
- 차원의 저주
- model을 정확히 알아야만 한다는 것
    - 하나의 state에서 action을 취할 때 얻게 되는 reward 그리고 transition matrix를 알아야만 계산이 가능하다. 당연하다. value function에 저 두개가 다 들어가기 때문이다.