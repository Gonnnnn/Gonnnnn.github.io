---
title: "RL 단단한 강화학습 Ch3 (2)"
categories:
    - TIL
tags:
    - CS
    - Reinforcement Learning
---
이렇게 기본적이고 사소한 것도 다시 읽으니 너무 새롭게 다가온다. 지금 내가 매우 어렵다고 난해하다고 생각하는 것들을 훗날 다시 마주했을 때 그로부터 얻을 수 있는 것은 얼마나 무한히 커져있을까????@!#!@#@!$@!$!@

완벽히 이해가 안된 부분들도 조금 있고, 잘못이해했을 수도 있다. 이번 공부의 요점은 부딪히기이기 때문에 빠르게 부딪히면서 강화학습에 대한 전반적인 이해도를 높이고, 경험을 통해 이해가 되지 않았던 부분들을 해결하며, 그 후 다시 공부하며 습득할 예정이다.

# Ch3 Finite Markov Decision Process

MDP에서는 행동이 이어지는 상황이나 상태에도 영향을 미쳐 미래의 보상에도 영향을 준다. 지연된 보상이 포함되며, 지연된 보상과 즉각적인 보상 사이에서 균형을 잡을 필요가 있다.

## 3.1 Agent-Environment Interface

- $S_t, R_t, A_t$가 소개된다. 에이전트와 환경에 대한 설명 정도는 구글에 차고 넘친다.
- Trajectory는 $S_0, A_0, R_1, S_1, A_1, R_2, ...$와 같이 time step마다 확인된 일련의 상태, 보상, 행동 집합이다. 유한 MDP에서는 이들은 항상 유한한 값의 숫자 원소를 갖는다.
- ***보상과 상태는 이전 상태와 행동에만 의존한다.***
$pr(s', r|s,a)\ \dot= \ Pr\{S_t=s', R_t=r|S_{t-1}=s, A_{t-1}=a\}$
***중요함!***
모든 $s \in S, a \in A(s)$에 대해 $\sum_{s' \in S} \sum_{r \in R}p(s',r|s,a) = 1$
    - 위 식을 r에 대해 적분하면
    $p(s'|s, a) = Pr\{S_t=s'|S_{t-1}=s, A_{t-1}=a\}$ 로, state-transition probability가 된다.
    - $r(s,a) = E[R_t|S_{t-1}=s, A_{t-1}=a] = \sum_{r \in R}r \sum_{s' \in S}p(s', r|s,a)$와 같이 적분해서 s, a를 선택했을 때 보상의 기댓값도 계산가능하다.
    - $r(s, a, s') = E[R_t|S_{t-1}=s, A_{t-1}=a, S_t = s'] = \sum_{r \in R}r{ p(s', r|s,a) \over p(s'|s,a)}$도 가능
    - 환경에 관한 모든 정보를 계산할 수 있는 식!

### MDP

- ***MDP는 세상최고짱이라서 유동적이다***. 예를 들어 time step이 반드시 고정된 간격을 나타낼 필요도 없다.
- ***행동은 결정하는 방법을 알고자 하는 모든 것, 상태는 그 결정을 내리는데 유용하다고 알려진 모든 것이 될 수 있다***! 에이전트에 의해 임의로 변경될 수 없는 모든 것은 환경이라고 생각하면 된다.
- 에이전트가 환경에 대해 몰라야 하는 것은 아니다. 에이전트는 루빅 큐브가 어떻게 작동하는지 알고 있지만, 풀 수 없는 것을 예로 생각할 수 있겠다.

## 3.2 목표와 보상

- 각 time step에서 받는 스칼라인 보상(reward, $r \in R)$의 총합, 즉 time step이 진행되며 누적되는 보상의 총합을 최대로 하는 것이 강화학습의 목표이다.
- 보상은 이루고자 하는 목표를 나타낸다. 다만, 목표에 ‘어떻게’ 도달할 것인지에 대한 사전 지식을 전달하는 것은 아니다.
    - 체스를 예로들면, 게임에서 이기는 경우에 대해서만 보상을 받아야지, 상대방의 말을 취하거나, 체스 판의 특정 지점을 장악하는 것과 같은 부차적인 경우에 대해서 보상을 받으면 안된다.
    - ***보상은 ‘무엇을’ 이루어야 하는지를 알려주는 방법이지, ‘어떻게’를 알려주는게 아니다.***

## 3.3 보상(reward)과 에피소드

- 누적 reward를 최대화하는 것이 강화학습의 목표였다. 강화학습에서는 이를 어떻게 수식적으로 표현해서 최대화를 할 수 있을까?? 다음과 같이 할 수 있다.
- Expected return(기대되는 이득)을 정의해서 누적 reward를 표현한다. Reward는 앞에서 추정해나갔던 그 reward, $Q$말하는거 맞다.
    - $G_t \ \dot= \ R_{t+1} + R_{t+2} + ... R_T$, $T$는 최종 time step
    - 게임과 같이 끝나는 지점이 있는 경우 최종 time step이 존재할 것이다. 이러한 한번의 시뮬레이션?을 에피소드라는 단위로 정의한다. 각 에피소드는 terminal state가 존재한다. 각 에피소드는 독립적으로 시작한다. 끝나면 초기화된다는 소리이다.
    - 대게 non terminal 상태를 제외한 모든 상태의 집합을 $S$, terminal 상태가 포함된 모든 상태 집합을 $S^+$로 표현한다.
    - 이득은 episode가 무한히 길어지면 적용하기에는 한계가 있는 식이다. 무한히 발산할 수 있기 때문이다.
- discounting을 정의한다.
    - $G_t \ \dot= \ \sum^\infin_{k=0} \gamma^kR_{t+k+1}, 0 \le \gamma \le 1$ $\gamma$는 할인율이다.
    - 할인율은 미래 보상의 현재 가치를 결정한다. 0이면 근시안적이고, 1에 가까울 수록 멀리 보는 것
- 둘은 다르긴한데, 다음과 같이 동시에 표현할 수 있다.
    - $G_t \ \dot= \ \sum^T_{k=t+1} \gamma^{k-t-1}R_k$
    - 무한히 확장되는 경우를 기본으로 한다. 하지만 episode가 끝나는 것을 마치 T번째 상태에서 action을 무엇을 취하든 계속해서 본 상태로 돌아오게 하고, 이 때 return을 0으로 설정하면 둘을 혼용해서 표현 가능하다.

## 3.5 Policy and Value function

- Value function(가치 함수)은 에이전트가 주어진 상태에 있는 것이 얼마나 좋은가(혹은 주어진 상태에서 주어진 행동을 수행하는 것이 얼마나 좋은가)를 추정하는 상태의 함수이다. 얼마나 좋은가는 미래의 보상, 즉 이득의 기댓값을 기준으로 정의된다. 따라서 당연히 가치 함수는 어떤 행동을 취할 것인가에 영향을 받고, policy(정책)이라는 것에 의해 정의된다.
- Policy : $\pi(a|s)$. 상태 s에서 a를 취할 확률
- 가치함수는 다음과 같이 정의된다.
    - 해당 상태가 얼마나 좋은지를 보여준다는 측면에서,
    모든 $s \in S$에 대해 $v_\pi(s) \ \dot= \ E_\pi[G_t | S_t=s]$
    가치 함수는, 상태 s에서 가능한 모든 이득의 평균인 셈이다.
    - 상태 s에서 행동 a를 취하는 것이 얼마나 좋은 것인지를 보여주는 측면에서,
    $q_\pi \ \dot= \ E_\pi[G_t|S_t=s, A_t=a]$
    이는 따로, action-value function이라고 칭한다.

여기까지 이 말을 안해서 긴가민가 했을 텐데, 가치 함수들은 경험을 통해서 추정하는 것이다.

- 정책 $\pi$를 따르고, 마주치는 모든 상태마다 얻을 수 있는 모든 이득의 평균 값을 기록한다면, 혹은 행동을 취함으로써 얻어지는 이득의 평균 값을 모두 기록한다면, 마주치는 상태의 개수가 무한으로 갈 때 상태의 가치가 실제 참 값으로 수렴할 것이다.
- 이렇게 하는게 monte carlo method이다. 상태나 행동이 매우 많다면 함수 근사를 사용해 표현하기도 한다.

### 재귀적 표현

- 가치 함수는 재귀적으로 표현된다.
- $v_\pi(s) \ \dot= \ E_\pi[G_t|S_t=s] = E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s]$
$=\sum_a\pi(a|s) \sum_{s',r}p(s',r|s,a)[r+\gamma E_\pi[G_{t+1}|S_{t+1}=s']]$
$=\sum_a\pi(a|s) \sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s')]$
- policy를 통해 현재 state s에서 a가 취해질 확률을 가져오고, state transition 확률 분포에서 s’, r이 나올 수 있는 확률을 가져와 가중치로 곱해줘서 []부분의 값의 평균을 구해준 것이다.
- 처음에 딱 보면 헷갈릴 수도 있는데,  backup diagram이란 것을 검색해보고, 그 구조대로 간단하게 예시를 그려보면 이해하기 쉬울 것이다.
- 참고로 위의 식은 $v_\pi$에 대한 bellman equation이다.
- 가치함수를 계산하고, 학습하며, 근삿값을 구하는 수많은 방법이 여기서 시작한다.

## 3.6 최적 정책과 최적 가치 함수

- 모든 $s$에 대해, $v_\pi(s) \geq v_{\pi'}(s)$일 때 $\pi$가 $\pi'$보다 좋은 것이다. 두 개 이상의 최적 정책이 있을 수 있다. 이런 최적 정책들을 합쳐서 $\pi_*$로 표현한다.
- $\pi_*$는 동일한 상태 가치 함수를 갖는다. 최적 정책이 갖는 상태 가치 함수를 최적 상태 가치 함수라고 한다.
$v_*(s) \ \dot= \ max_{\pi} v_\pi(s)$.
- 마찬가지로 $q_*(s,a) \ \dot= \ max_{\pi} q_\pi(s,a)$
상태 s에서 행동 a를 선택한 후, 최적 정책을 따를 때 얻게 될 이득의 기댓값을 도출하는 것을 의미하므로,
$q_*(s,a) = E[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s, A_t=a]$로 표현할 수 있다.

### 최적 벨만 방정식

$v_*$는 특정 정책과는 상관없는 형태로 표현할 수 있고, 이를 최적 벨만 방정식, $v_*$를 구하기 위한 방정식 이라고 한다. 특정 정책과는 상관 없다는 것은 매 상태에서 이득을 최대화 하는 방향으로만 이득을 계산한 것이 바로 최적이기 때문이다.

- $v_*(s) = max_{a \in A(s)} q_{\pi_*}(s,a)$
$= max_a \ E_{\pi_*}[G_t|S_t=s, A_t=a]$
$= max_a \ E_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s, A_t=a]$
$= max_a \ E_{\pi_*}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s, A_t=a]$
$=max_a \ \sum_{s', r}p(s',r|s,a)[r+\gamma v_*(s')]$
- 최적 정책을 따르는 상태의 가치는 그 상태에서 선택할 수 있는 가장 좋은 행동으로부터 나오는 이득의 기댓값과 같아야 한다는 것이 최적 벨만 방정식이 나타내는 바이다.
- 최적 벨만 방정식은 최적 가치 함수가 만족시켜야하는 일관성에 대하 조건이다.
- $q_*$를 구하기 위한 방정식은.. 귀찮아서 안적는다. 이정도는 혼자서 할 수 있어야지

### 최적 정책

- 일단 $v_*$를 알면, 최적 정책을 결정하는 것은 쉽다. s에 대해 최적 벨만 ㅂ아정식의 가치 함수가 최댓값이 되도록 하는 행동이 하나 이상은 있을 것이다. 이러한 행동에 대해서만 0이 아닌 확률을 부여하는 정책은 무엇이든 최적 정책이 된다.
- 다른 말로는, 최적 가치 함수에 대해 탐욕적인 정책은 최적 정책이라는 것이다.
- 행동의 단기적 결과를 평가하기 위해 가치함수 $v_*$를 사용하면, 자연스럽게 탐욕적 정책이 장기적인 측면에서의 최적의 결과를 가져온다.
- 신기하다.

## 3.7 최적성과 근사

- 하지만 단일 time step에서 수행해야 하는 계산의 양이 많거나, 모델을 완벽히 정의할 수 없고, state, action이 매우 많은 경우 등 위 방법을 제대로 적용할 수 없는 경우가 많다.
- 뭐 다른건 모르겠고
- 강화학습의 온라인적 특성은 자주 마주치는 상태에는 학습에 노력을 많이 기울이고, 그렇지 않은 상태는 덜 기울이는 방법으로 정책을 근사한다. 이 것이 MDP 문제를 근사적으로 해결하는 다른 방법들과 강화학습의 차이점이라고 한다.
- 실제 에이전트는 다양한 수준에서 근삿값을 구할 수 있을 뿐이라고 한다.
